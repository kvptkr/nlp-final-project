{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cefcef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from utils.dataset import DataSet\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats, word_overlap_features, get_tokenized_lemmas\n",
    "training_dataset = DataSet(\"train\")\n",
    "testing_dataset = DataSet(\"competition_test\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "# competition_dataset\n",
    "\n",
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a5da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, b, y = [],[],[]\n",
    "name = \"comp\"\n",
    "x = 0\n",
    "training_df = training_dataset.to_dataframe()\n",
    "training_df.head()\n",
    "\n",
    "testing_df = testing_dataset.to_dataframe()\n",
    "\n",
    "# X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "# X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "# X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "# X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "\n",
    "# X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68022fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()\n",
    "\n",
    "\n",
    "# Tokenizing and lemmatizing everything\n",
    "training_df['processed_headlines'] = training_df.apply(lambda row: get_tokenized_lemmas(row['Headlines']), axis=1)\n",
    "training_df['processed_bodies'] = training_df.apply(lambda row: get_tokenized_lemmas(row['Bodies']), axis=1)\n",
    "\n",
    "testing_df['processed_headlines'] = testing_df.apply(lambda row: get_tokenized_lemmas(row['Headlines']), axis=1)\n",
    "testing_df['processed_bodies'] = testing_df.apply(lambda row: get_tokenized_lemmas(row['Bodies']), axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21130ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stances</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Bodies</th>\n",
       "      <th>processed_headlines</th>\n",
       "      <th>processed_bodies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>ferguson riots pregnant woman loses eye after ...</td>\n",
       "      <td>a respected senior french police officer inves...</td>\n",
       "      <td>ferguson riot pregnant woman loses eye after c...</td>\n",
       "      <td>a respected senior french police officer inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>crazy conservatives are sure a gitmo detainee ...</td>\n",
       "      <td>dave morin s social networking company path is...</td>\n",
       "      <td>crazy conservative are sure a gitmo detainee k...</td>\n",
       "      <td>dave morin s social networking company path is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>a russian guy says his justin bieber ringtone ...</td>\n",
       "      <td>a bereaved afghan mother took revenge on the t...</td>\n",
       "      <td>a russian guy say his justin bieber ringtone s...</td>\n",
       "      <td>a bereaved afghan mother took revenge on the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>zombie cat buried kitty believed dead meows ba...</td>\n",
       "      <td>hewlett packard is officially splitting in two...</td>\n",
       "      <td>zombie cat buried kitty believed dead meow bac...</td>\n",
       "      <td>hewlett packard is officially splitting in two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>argentina s president adopts boy to end werewo...</td>\n",
       "      <td>an airline passenger headed to dallas was remo...</td>\n",
       "      <td>argentina s president adopts boy to end werewo...</td>\n",
       "      <td>an airline passenger headed to dallas wa remov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stances                                          Headlines  \\\n",
       "0        3  ferguson riots pregnant woman loses eye after ...   \n",
       "1        3  crazy conservatives are sure a gitmo detainee ...   \n",
       "2        3  a russian guy says his justin bieber ringtone ...   \n",
       "3        3  zombie cat buried kitty believed dead meows ba...   \n",
       "4        3  argentina s president adopts boy to end werewo...   \n",
       "\n",
       "                                              Bodies  \\\n",
       "0  a respected senior french police officer inves...   \n",
       "1  dave morin s social networking company path is...   \n",
       "2  a bereaved afghan mother took revenge on the t...   \n",
       "3  hewlett packard is officially splitting in two...   \n",
       "4  an airline passenger headed to dallas was remo...   \n",
       "\n",
       "                                 processed_headlines  \\\n",
       "0  ferguson riot pregnant woman loses eye after c...   \n",
       "1  crazy conservative are sure a gitmo detainee k...   \n",
       "2  a russian guy say his justin bieber ringtone s...   \n",
       "3  zombie cat buried kitty believed dead meow bac...   \n",
       "4  argentina s president adopts boy to end werewo...   \n",
       "\n",
       "                                    processed_bodies  \n",
       "0  a respected senior french police officer inves...  \n",
       "1  dave morin s social networking company path is...  \n",
       "2  a bereaved afghan mother took revenge on the t...  \n",
       "3  hewlett packard is officially splitting in two...  \n",
       "4  an airline passenger headed to dallas wa remov...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e903f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrix'>\n",
      "(49972, 3093)\n",
      "(25413, 2116)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Getting the TFs for Training data\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "headlines_count = cv.fit_transform(training_df['processed_headlines'])\n",
    "bodies_count = cv.transform(training_df['processed_bodies'])\n",
    "\n",
    "headlines_tf = TfidfTransformer(use_idf=False).fit_transform(headlines_count)\n",
    "bodies_tf = TfidfTransformer(use_idf=False).transform(bodies_count)\n",
    "\n",
    "# Getting the TFs for Testing Data\n",
    "headlines_count_test = cv.transform(testing_df['processed_headlines'])\n",
    "bodies_count_test = cv.transform(testing_df['processed_bodies'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "headlines_tf = tfidf_transformer.fit_transform(headlines_count)\n",
    "bodies_tf = tfidf_transformer.transform(bodies_count)\n",
    "\n",
    "headlines_tf_test  = tfidf_transformer.transform(headlines_count_test)\n",
    "bodies_tf_test = tfidf_transformer.transform(bodies_count_test)\n",
    "\n",
    "\n",
    "headline_vectors = tfidf_vectorizer.fit_transform(training_df['processed_headlines']).todense()\n",
    "body_vectors = tfidf_vectorizer.transform(training_df['processed_bodies'])\n",
    "\n",
    "headline_vectors_test = tfidf_vectorizer.fit_transform(testing_df['processed_headlines']).todense()\n",
    "body_vectors_test = tfidf_vectorizer.transform(testing_df['processed_bodies'])\n",
    "\n",
    "\n",
    "print(type(headline_vectors))\n",
    "print(headline_vectors.shape)\n",
    "print(body_vectors_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a7160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49972x3093 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 538152 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = headlines_count[0].toarray()[0]\n",
    "headlines_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40da843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "cosine_similarity_list = []\n",
    "cosine_similarity_list_test = []\n",
    "\n",
    "full_feature_train = []\n",
    "full_feature_test = []\n",
    "for i in range(headline_vectors.shape[0]):\n",
    "    cosine_similarity_temp = cosine_similarity(headline_vectors[i], body_vectors[i])\n",
    "    cosine_similarity_list.append(cosine_similarity_temp)\n",
    "    headline_tf_vector = headlines_count[i].toarray()[0]\n",
    "    body_tf_vector = bodies_count[i].toarray()[0]\n",
    "    full_feature = np.hstack(( cosine_similarity_temp[0],headline_tf_vector,body_tf_vector )).ravel()\n",
    "    full_feature_train.append(full_feature)\n",
    "    \n",
    "for i in range(headline_vectors_test.shape[0]):\n",
    "    cosine_similarity_temp = cosine_similarity(headline_vectors_test[i], body_vectors_test[i])\n",
    "    cosine_similarity_list_test.append(cosine_similarity_temp)\n",
    "    headline_tf_vector = headlines_count[i].toarray()[0]\n",
    "    body_tf_vector = bodies_count[i].toarray()[0]\n",
    "    full_feature = np.hstack(( cosine_similarity_temp[0],headline_tf_vector,body_tf_vector )).ravel()\n",
    "    full_feature_test.append(full_feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cs_l = []\n",
    "# for element in cosine_similarity_list:\n",
    "#     cs_l.append(element[0][0])\n",
    "\n",
    "\n",
    "# cs_l = np.array(cs_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a1fc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_feature_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0206f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 6187)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_l_test = []\n",
    "# for element in consine_similarity_list_test:\n",
    "#     cs_l_test.append(element[0][0])\n",
    "    \n",
    "# cs_l = np.array(cs_l)\n",
    "# cs_l = np.array(cs_l)\n",
    "# cs_l.shape\n",
    "\n",
    "cs_l_test = np.array(cs_l_test)\n",
    "\n",
    "full_feature = np.array(full_feature_train)\n",
    "full_feature_test_1 = np.array(full_feature_test)\n",
    "full_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a452359",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cs_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pq/yvf1g_097190wr2_pw5l46040000gn/T/ipykernel_2238/762327344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cs_l' is not defined"
     ]
    }
   ],
   "source": [
    "print(cs_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87976142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_categorical = np_utils.to_categorical(training_df['Stances'])\n",
    "y_test_categorical = np_utils.to_categorical(testing_df['Stances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500, input_shape=(6187,)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(500))\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs_l.shape\n",
    "model.fit(full_feature, y_train_categorical, batch_size=128, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(full_feature_test_1, y_test_categorical, batch_size=1)\n",
    "# predictions = model.predict(full_feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.score import report_score, generate_answer_csv\n",
    "predicted = []\n",
    "print(predictions.shape)\n",
    "for prediction in predictions:\n",
    "    max_val = np.argmax(prediction)\n",
    "    predicted.append(max_val)\n",
    "predicted_vals = [LABELS[int(a)] for a in predicted]\n",
    "actual = [LABELS[int(a)] for a in testing_df['Stances']]\n",
    "\n",
    "\n",
    "\n",
    "report_score(actual, predicted_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
